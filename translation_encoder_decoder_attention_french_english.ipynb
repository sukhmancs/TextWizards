{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sukhmancs/TextWizards/blob/main/translation_encoder_decoder_attention_french_english.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4G1Ek8wFt-y"
      },
      "outputs": [],
      "source": [
        "#!pip install ipympl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATED\n",
        "###########################################################\n",
        "#!pip install gensim==4.3.1\n",
        "# The library has been archived and won't be used anymore\n",
        "# # !pip install allennlp==0.9.0\n",
        "#!pip install flair==0.12.2\n",
        "#!pip install torchvision==0.15.1\n",
        "# # HuggingFace\n",
        "!pip install transformers==4.32.0\n",
        "!pip install datasets==2.14.4\n",
        "###########################################################"
      ],
      "metadata": {
        "id": "3BI2lbS0VPK9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lam4s72f4_2"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "#%matplotlib inline\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFiDgFY43Cef"
      },
      "source": [
        "# **IMPORT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lmjl3f7nFTKD"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "try:\n",
        "    import google.colab\n",
        "    import requests\n",
        "    url = 'https://raw.githubusercontent.com/dvgodoy/PyTorchStepByStep/master/config.py'\n",
        "    r = requests.get(url, allow_redirects=True)\n",
        "    open('config.py', 'wb').write(r.content)\n",
        "except ModuleNotFoundError:\n",
        "    pass\n",
        "\n",
        "from config import *\n",
        "config_chapter9()\n",
        "# This is needed to render the plots in this chapter\n",
        "from plots.chapter8 import *\n",
        "from plots.chapter9 import *\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YkwTFYIhFTKE"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset, RandomSampler\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import re\n",
        "import random\n",
        "\n",
        "#from data_generation.square_sequences import generate_sequences\n",
        "#from stepbystep.v4 import StepByStep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_b7XyG6pl_W"
      },
      "outputs": [],
      "source": [
        "#!pip install trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NPhjRQaogx_"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "from termcolor import colored\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import trax\n",
        "from trax import layers as tl\n",
        "from trax.fastmath import numpy as fastnp\n",
        "from trax.supervised import training\n",
        "\n",
        "!pip list | grep trax\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFdjE0eC2l7T"
      },
      "source": [
        "# __Start here:__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWJ7tZNlt7-K"
      },
      "outputs": [],
      "source": [
        "cd /content/drive/MyDrive/Code_files_and_data/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCOvFlBfQK1U"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfK2fzq52lk0"
      },
      "outputs": [],
      "source": [
        "# Download the data from this pytorch tutorial https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html#loading-data-files\n",
        "# or Download directly by following this link https://download.pytorch.org/tutorial/data.zip\n",
        "import os\n",
        "\n",
        "#zip_file_path = '/content/drive/MyDrive/Code_files_and_data/data/data.zip' # Uncomment for For french to english dataset\n",
        "zip_file_path = '/content/drive/MyDrive/Code_files_and_data/data/data_vie.zip'  # Uncomment for Vietnamese to english dataset\n",
        "extracted_dir_path = '/content/drive/MyDrive/Code_files_and_data/data/data/'\n",
        "\n",
        "# Check if the extracted directory already exists\n",
        "if not os.path.exists(extracted_dir_path):\n",
        "    # If not, then unzip the file\n",
        "    !unzip $zip_file_path\n",
        "else:\n",
        "    print(f\"The directory {extracted_dir_path} already exists. No need to extract.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlfdNQuKhDTY"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate Only the data (Non preprocessed embeddings steps)"
      ],
      "metadata": {
        "id": "0u7_zLW0LpDE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBB4zRME3KzL"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p71SzQiJ3rLN"
      },
      "outputs": [],
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
        "    return s.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkm3DGDt37_8"
      },
      "outputs": [],
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]  # Remove me - To train on entire data change this `for l in lines[:5000]` to `for l in lines`\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = Lang(lang2)\n",
        "        output_lang = Lang(lang1)\n",
        "    else:\n",
        "        input_lang = Lang(lang1)\n",
        "        output_lang = Lang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-53sCMxTNOWC"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wX4uFmKHNOWE"
      },
      "outputs": [],
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-vL7tHG56q1"
      },
      "outputs": [],
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)\n",
        "\n",
        "def get_dataloader(batch_size):\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    n = len(pairs)\n",
        "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    for idx, (inp, tgt) in enumerate(pairs):\n",
        "        inp_ids = indexesFromSentence(input_lang, inp)\n",
        "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "        inp_ids.append(EOS_token)\n",
        "        tgt_ids.append(EOS_token)\n",
        "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
        "                               torch.LongTensor(target_ids).to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRtnxmuqz2rq"
      },
      "outputs": [],
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "MAX_LENGTH = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sZeORww3z2rr"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtvCZxF66Ejw"
      },
      "outputs": [],
      "source": [
        "input_lang, output_lang, train_dataloader = get_dataloader(batch_size=32)\n",
        "\n",
        "for data in train_dataloader:\n",
        "    inputs, targets = data\n",
        "    print(inputs)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PreProcessed Embeddings (old steps)"
      ],
      "metadata": {
        "id": "QozLmhUZ3KMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_sentences, french_sentences = zip(*pairs)\n",
        "\n",
        "print(\"English sentences:\", list(english_sentences))\n",
        "print(\"French sentences:\", list(french_sentences))"
      ],
      "metadata": {
        "id": "zV4kXZAtajrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpdPQQMSjsZg"
      },
      "outputs": [],
      "source": [
        "#from flair.data import Sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TuHubbejsZi"
      },
      "outputs": [],
      "source": [
        "# from flair.embeddings import TransformerWordEmbeddings\n",
        "# bert = TransformerWordEmbeddings('bert-base-uncased', layers='-1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNF-mujajsZm"
      },
      "outputs": [],
      "source": [
        "# def get_embeddings(embeddings, sentence):\n",
        "#     sent = Sentence(sentence)\n",
        "#     embeddings.embed(sent)\n",
        "#     return torch.stack([token.embedding for token in sent.tokens]).float()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_dataset_doc = [get_embeddings(bert, row) for row in english_sentences]"
      ],
      "metadata": {
        "id": "NkBiJz23k2b4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#torch.stack(train_dataset_doc) # Embeddings for each word in a sentence\n",
        "# ToDo: these embeddings have different sequence length because each sentence had different length, so do something about it."
      ],
      "metadata": {
        "id": "6t4d2NiloRgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preproccessing embedding (New steps)-\n",
        "Only run this cell if you are using **Encoder Decoder using preprocessed embeddings cell**"
      ],
      "metadata": {
        "id": "EpqWzSuyAt_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "Mz-TqIjvApxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "model_bert = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True,)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "SPgfP9sdAtH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LENGTH = 10"
      ],
      "metadata": {
        "id": "yfPz1fhRUKH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"he eventually sold the shares back to the bank at a premium.\"\n",
        "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "tokenized_text_1 = tokenizer.tokenize(marked_text)\n",
        "tokenized_text = tokenizer(text, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\") # automatically adds [CLS] and [SEP] tokens\n",
        "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text_1)\n",
        "\n",
        "print(f\"tokenized_text: {tokenized_text_1}, \\ntokenized_text: {tokenized_text['input_ids']}, \\ntokenized_text_1 indexed tokens: {indexed_tokens}\")"
      ],
      "metadata": {
        "id": "x_lptnF3TWtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bert_text_preparation(text, tokenizer):\n",
        "  \"\"\"\n",
        "  Preprocesses text input in a way that BERT can interpret.\n",
        "  \"\"\"\n",
        "  #marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
        "  #tokenized_text = tokenizer.tokenize(marked_text)\n",
        "  #indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "  #segments_ids = [1]*len(indexed_tokens)\n",
        "  # convert inputs to tensors\n",
        "  #tokens_tensor = torch.tensor([indexed_tokens])\n",
        "  tokens_tensor = tokenizer(text, max_length=MAX_LENGTH, padding=\"max_length\", truncation=True, return_tensors=\"pt\") # automatically adds [CLS] and [SEP] tokens\n",
        "  #segments_tensor = torch.tensor([segments_ids])\n",
        "  return tokens_tensor['input_ids']"
      ],
      "metadata": {
        "id": "zIaCjpgBAzvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bert_embeddings(tokens_tensor, model):\n",
        "    \"\"\"\n",
        "    Obtains BERT embeddings for tokens.\n",
        "    \"\"\"\n",
        "    # gradient calculation id disabled\n",
        "    with torch.no_grad():\n",
        "      # obtain hidden states\n",
        "      outputs = model(tokens_tensor)\n",
        "      hidden_states = outputs[2]    # concatenate the tensors for all layers\n",
        "    # use \"stack\" to create new dimension in tensor\n",
        "    token_embeddings = torch.stack(hidden_states, dim=0)    # remove dimension 1, the \"batches\"\n",
        "    token_embeddings = torch.squeeze(token_embeddings, dim=1)    # swap dimensions 0 and 1 so we can loop over tokens\n",
        "    #print(f\"token_embeddings_stacked_squeezed size: {token_embeddings.size()}\")\n",
        "    token_embeddings = token_embeddings.permute(1,0,2)    # intialized list to store embeddings\n",
        "    token_vecs_sum = []    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
        "    # where Y is the number of tokens in the sentence    # loop over tokens in sentence\n",
        "    for token in token_embeddings:    # \"token\" is a [12 x 768] tensor    # sum the vectors from the last four layers\n",
        "        sum_vec = torch.sum(token[-4:], dim=0)\n",
        "        token_vecs_sum.append(sum_vec)\n",
        "    return token_vecs_sum"
      ],
      "metadata": {
        "id": "GHLEhGz3A5Z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"he eventually sold the shares back to the bank at a premium.\"\n",
        "tokens_tensor = bert_text_preparation(sentence, tokenizer)\n",
        "with torch.no_grad():\n",
        "      # obtain hidden states\n",
        "      outputs = model_bert(tokens_tensor)\n",
        "      hidden_states = outputs[2]\n",
        "# use \"stack\" to create new dimension in tensor\n",
        "token_embeddings = torch.stack(hidden_states, dim=0)    # remove dimension 1, the \"batches\"\n",
        "token_embeddings = torch.squeeze(token_embeddings, dim=1)    # swap dimensions 0 and 1 so we can loop over tokens\n",
        "#print(f\"token_embeddings_stacked_squeezed size: {token_embeddings.size()}\")\n",
        "token_embeddings = token_embeddings.permute(1,0,2)\n",
        "token_vecs_sum = []    # \"token_embeddings\" is a [Y x 12 x 768] tensor\n",
        "    # where Y is the number of tokens in the sentence    # loop over tokens in sentence\n",
        "for token in token_embeddings:    # \"token\" is a [12 x 768] tensor    # sum the vectors from the last four layers\n",
        "    sum_vec = torch.sum(token[-4:], dim=0)\n",
        "    token_vecs_sum.append(sum_vec)\n",
        "print(len(token_vecs_sum)) # A list of token embeddings for each sentence"
      ],
      "metadata": {
        "id": "gAHJf4-_l_3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"bank\",\n",
        "         \"he eventually sold the shares back to the bank at a premium.\",\n",
        "         \"the bank strongly resisted cutting interest rates.\",\n",
        "         \"the bank will supply and buy back foreign currency.\",\n",
        "         \"the bank is pressing us for repayment of the loan.\",\n",
        "         \"the bank left its lending rates unchanged.\",\n",
        "         \"the river flowed over the bank.\",\n",
        "         \"tall, luxuriant plants grew along the river bank.\",\n",
        "         \"his soldiers were arrayed along the river bank.\",\n",
        "         \"wild flowers adorned the river bank.\",\n",
        "         \"two fox cubs romped playfully on the river bank.\",\n",
        "         \"the jewels were kept in a bank vault.\",\n",
        "         \"you can stow your jewellery away in the bank.\",\n",
        "         \"most of the money was in storage in bank vaults.\",\n",
        "         \"the diamonds are shut away in a bank vault somewhere.\",\n",
        "         \"thieves broke into the bank vault.\",\n",
        "         \"can I bank on your support?\",\n",
        "         \"you can bank on him to hand you a reasonable bill for your services.\",\n",
        "         \"don't bank on your friends to help you out of trouble.\",\n",
        "         \"you can bank on me when you need money.\",\n",
        "         \"i bank on your help.\"\n",
        "         ]\n",
        "\n",
        "from collections import OrderedDict\n",
        "context_embeddings = []\n",
        "context_tokens = []\n",
        "count = 0\n",
        "\n",
        "for sentence in sentences:\n",
        "  tokens_tensor = bert_text_preparation(sentence, tokenizer)\n",
        "  #print(f\"tokens_tensor: {tokens_tensor}\")\n",
        "  list_token_embeddings = get_bert_embeddings(tokens_tensor, model_bert)  # make ordered dictionary to keep track of the position of each   word\n",
        "  context_embeddings.append(list_token_embeddings)\n",
        "  #print(f\"list_token_embedding: {list_token_embeddings}\")\n",
        "\n",
        "  # Remove me\n",
        "  #if count == 2: break\n",
        "  #count += 1\n",
        "\n",
        "#   tokens = OrderedDict()  # loop over tokens in sensitive sentence\n",
        "#   for token in tokenized_text[1:-1]:\n",
        "#     # keep track of position of word and whether it occurs multiple times\n",
        "#     if token in tokens:\n",
        "#       tokens[token] += 1\n",
        "#     else:\n",
        "#       tokens[token] = 1  # compute the position of the current token\n",
        "#     token_indices = [i for i, t in enumerate(tokenized_text) if t == token]\n",
        "#     current_index = token_indices[tokens[token]-1]  # get the corresponding embedding\n",
        "#     token_vec = list_token_embeddings[current_index]\n",
        "\n",
        "#     # save values\n",
        "#     context_tokens.append(token)\n",
        "#    context_embeddings.append(list_token_embeddings)"
      ],
      "metadata": {
        "id": "pidELh40BBXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the list of lists of tensors to a three-dimensional tensor\n",
        "context_embeddings_tensor = torch.stack([torch.stack(embeddings) for embeddings in context_embeddings], dim=0)\n",
        "\n",
        "print(context_embeddings_tensor.shape)  # This will be of shape (N, L, H)"
      ],
      "metadata": {
        "id": "W7gB5WhwwSBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "print(random.choice(pairs))"
      ],
      "metadata": {
        "id": "rXemiLnm_5gZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [pair[0] for pair in pairs]\n",
        "print(inputs[:40])"
      ],
      "metadata": {
        "id": "9XnwECZQ_6ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = [pair[0] for pair in pairs]\n",
        "context_embeddings = []\n",
        "context_tokens = []\n",
        "count = 0\n",
        "\n",
        "for sentence in inputs[:20]:\n",
        "    tokens_tensor = bert_text_preparation(sentence, tokenizer)\n",
        "        #print(f\"tokens_tensor: {tokens_tensor}\")\n",
        "    list_token_embeddings = get_bert_embeddings(tokens_tensor, model)  # make ordered dictionary to keep track of the position of each   word\n",
        "    context_embeddings.append(list_token_embeddings)\n",
        "\n",
        "context_embeddings_tensor = torch.stack([torch.stack(embeddings) for embeddings in context_embeddings], dim=0)\n",
        "    #print(f\"context_embeddings_tensor: {context_embeddings_tensor}\")\n",
        "train_data = TensorDataset(context_embeddings_tensor.to(device))\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=4)"
      ],
      "metadata": {
        "id": "QlYuE9Cx_NAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for data in train_dataloader:\n",
        "    print(data[0].size())\n",
        "    break"
      ],
      "metadata": {
        "id": "YFKKX-GrEBIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_embeddings(n_pairs=None):\n",
        "\n",
        "    #input_lang, output_lang, pairs = readLangs('eng', 'fra', True)\n",
        "    input_lang, output_lang, pairs = prepareData('eng', 'fra', True)\n",
        "\n",
        "    if n_pairs is not None:\n",
        "        pairs = pairs[:n_pairs]  # remove me\n",
        "\n",
        "    # Extract French sentences into a separate list\n",
        "    #input_sentences = [pair[0] for pair in pairs]\n",
        "    #target_sentences = [pair[1] for pair in pairs]\n",
        "\n",
        "    context_embeddings = []\n",
        "    input_context_tokens = []\n",
        "    target_context_tokens = []\n",
        "    count = 0\n",
        "\n",
        "    for input_sentence, target_sentence in pairs:\n",
        "        input_tokens_tensor = bert_text_preparation(input_sentence, tokenizer)\n",
        "        target_tokens_tensor = bert_text_preparation(target_sentence, tokenizer)\n",
        "        #print(f\"tokens_tensor: {tokens_tensor}\")\n",
        "        list_token_embeddings = get_bert_embeddings(input_tokens_tensor, model)  # make ordered dictionary to keep track of the position of each   word\n",
        "        #print(f\"input_tokens_tensor:\\n{input_tokens_tensor},\\ntarget_tokens_tensor:\\n{target_tokens_tensor}\")\n",
        "        context_embeddings.append(list_token_embeddings)\n",
        "        input_context_tokens.append(input_tokens_tensor)\n",
        "        target_context_tokens.append(target_tokens_tensor)\n",
        "\n",
        "    context_embeddings_tensor = torch.stack([torch.stack(embeddings) for embeddings in context_embeddings], dim=0)\n",
        "\n",
        "    # Convert the list of tensors to a three-dimensional tensor\n",
        "    input_context_tokens_tensor = torch.stack(input_context_tokens, dim=0)\n",
        "    target_context_tokens_tensor = torch.stack(target_context_tokens, dim=0)\n",
        "\n",
        "    # Remove the middle dimension with size 1\n",
        "    input_context_tokens_tensor = input_context_tokens_tensor.squeeze(dim=1)\n",
        "    target_context_tokens_tensor = target_context_tokens_tensor.squeeze(dim=1)\n",
        "\n",
        "    return context_embeddings_tensor, input_context_tokens_tensor, target_context_tokens_tensor"
      ],
      "metadata": {
        "id": "GXMILg3BVzb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_embeddings_tensor, input_context_tokens_tensor, target_context_tokens_tensor = generate_embeddings(n_pairs=10)"
      ],
      "metadata": {
        "id": "H5jGjTBRWOiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"input_context_tokens_tensor:\\n{input_context_tokens_tensor}, \\ntarget_context_tokens_tensor:\\n{target_context_tokens_tensor}\")"
      ],
      "metadata": {
        "id": "ypF5L2H1cMkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preprocessed embeddings dataloader\n",
        "\n",
        "def get_dataloader(batch_size, n_pairs=None):\n",
        "    context_embeddings_tensor, input_context_tokens_tensor, target_context_tokens_tensor = generate_embeddings(n_pairs)\n",
        "\n",
        "    print(\"worked so far\")\n",
        "    #target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
        "\n",
        "    # Extract French sentences into a separate list\n",
        "    #input_sentences = [pair[0] for pair in pairs]\n",
        "\n",
        "    #sentences = []\n",
        "    # for idx, (inp, tgt) in enumerate(pairs):\n",
        "    #     #inp_ids = indexesFromSentence(input_lang, inp)\n",
        "    #     tgt_ids = indexesFromSentence(output_lang, tgt)\n",
        "    #     #inp_ids.append(EOS_token)\n",
        "    #     #sentences.append(inp)\n",
        "    #     tgt_ids.append(EOS_token)\n",
        "    #     #input_ids[idx, :len(inp_ids)] = inp_ids\n",
        "    #     target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
        "\n",
        "    # context_embeddings = []\n",
        "    # context_tokens = []\n",
        "    # count = 0\n",
        "\n",
        "    # for sentence in input_sentences:\n",
        "    #     tokens_tensor = bert_text_preparation(sentence, tokenizer)\n",
        "    #     #print(f\"tokens_tensor: {tokens_tensor}\")\n",
        "    #     list_token_embeddings = get_bert_embeddings(tokens_tensor, model)  # make ordered dictionary to keep track of the position of each   word\n",
        "    #     context_embeddings.append(list_token_embeddings)\n",
        "\n",
        "    # context_embeddings_tensor = torch.stack([torch.stack(embeddings) for embeddings in context_embeddings], dim=0)\n",
        "    #print(f\"context_embeddings_tensor: {context_embeddings_tensor}\")\n",
        "    #train_data = TensorDataset(context_embeddings_tensor.to(device),\n",
        "     #                          torch.LongTensor(target_ids.view(-1)).to(device))\n",
        "    train_data = TensorDataset(input_context_tokens_tensor.to(device), target_context_tokens_tensor.to(device))\n",
        "\n",
        "    train_sampler = RandomSampler(train_data)\n",
        "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "    return input_lang, output_lang, train_dataloader, context_embeddings_tensor.view(-1, context_embeddings_tensor.size(-1))"
      ],
      "metadata": {
        "id": "rbGl7P0uzWUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_lang, output_lang, train_dataloader, context_embeddings_tensor = get_dataloader(batch_size=4, n_pairs=10)\n",
        "\n",
        "for data in train_dataloader:\n",
        "    inputs, targets = data\n",
        "    print(f\"inputs type: {type(inputs)}, \\ntargets types: {type(targets)}\")\n",
        "    print(f\"inputs size: {inputs.size()}, \\ntargets size: {targets.size()}\")\n",
        "    print(f\"input_tokens:\\n{inputs}, \\ntarget_tokens:\\n{targets}\")\n",
        "    #print(f\"last input dimension: {inputs.size(-1)}\")\n",
        "    #print(f\"reshaped input dimension: {inputs.view(-1, inputs.size(-1)).size()}\")\n",
        "    break"
      ],
      "metadata": {
        "id": "pDFdVz1pzIj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# filepath = os.path.join('/content/drive/MyDrive/Code_files_and_data/data/projections/')\n",
        "# name = 'metadata_small.tsv'\n",
        "# with open(os.path.join(filepath, name), 'w+') as file_metadata:\n",
        "#   for i, token in enumerate(context_tokens):\n",
        "#     file_metadata.write(token + '\\n')\n",
        "\n",
        "# import csv\n",
        "# name = 'embeddings_small.tsv'\n",
        "# with open(os.path.join(filepath, name), 'w+') as tsvfile:\n",
        "#     writer = csv.writer(tsvfile, delimiter='\\t')\n",
        "#     for embedding in context_embeddings:\n",
        "#         writer.writerow(embedding.numpy())\n",
        "\n",
        "# ## Use the data stored in /content/drive/MyDrive/Code_files_and_data/data/projections/ folder and upload it to tensorboard to visualize"
      ],
      "metadata": {
        "id": "3l6B1X7fBqKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uK-4WM6MEYpG"
      },
      "source": [
        "# Encoder-Decoder with Attention and RNN\n",
        "Note: I have also included code for simple Decoder that do not use attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTOl8Wvbg4-f"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYpgN1FlFTKR"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_features, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_features = n_features\n",
        "        self.hidden = None\n",
        "        #self.embedding_dimension = 256\n",
        "        self.embd = nn.Embedding(self.n_features, self.hidden_dim)\n",
        "        self.basic_rnn = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X_embd = self.embd(X) # N, F -> N, F, H\n",
        "        rnn_out, self.hidden = self.basic_rnn(X_embd) # N, F, H x N, H, H  ->  N, F, H\n",
        "\n",
        "        return rnn_out, self.hidden # N, F, H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6Npk2H3Zu6D"
      },
      "outputs": [],
      "source": [
        "embd = nn.Embedding(5, 5)\n",
        "rnn = nn.GRU(5, 5, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1miZ7VQPZgQl"
      },
      "outputs": [],
      "source": [
        "full_seq = torch.full((2, 3), 1)\n",
        "rnn_out, final_hidden = rnn(embd(full_seq))\n",
        "final_hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ0_fIFigvMt"
      },
      "source": [
        "### Testig Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDWlPmIQFTKS"
      },
      "outputs": [],
      "source": [
        "#torch.manual_seed(21)\n",
        "full_seq = torch.full((2, 3), 1).to(device)\n",
        "encoder = Encoder(n_features=3, hidden_dim=5).to(device)\n",
        "hidden_seq, hidden_final = encoder(full_seq) # output is N, L, F\n",
        "hidden_final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEuyhiH3FTKS"
      },
      "source": [
        "## Simple Decoder (no Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKijt9R3FTKS"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/decoder.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xXs9hsKkFTKT"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.hidden = None\n",
        "        self.embedding = nn.Embedding(output_size, self.hidden_dim)\n",
        "        self.basic_rnn = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
        "        self.regression = nn.Linear(self.hidden_dim, output_size)\n",
        "\n",
        "    def init_hidden(self, hidden_final):\n",
        "        # We only need the final hidden state from encoder for each sentence\n",
        "        #hidden_final = hidden_seq[:, -1:] # N, F\n",
        "        self.hidden = hidden_final\n",
        "        #self.hidden = hidden_final.permute(1, 0, 2) # 1, N, H\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.embedding(X) # N, 1 -> N, F\n",
        "        batch_first_output, self.hidden = self.basic_rnn(X, self.hidden) # N, F\n",
        "        out = self.regression(batch_first_output) # N, output_size\n",
        "\n",
        "        # N, 1, F\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fKXfgWcgXPd"
      },
      "source": [
        "### Testing Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfLh-gpxFTKT"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(21)\n",
        "decoder = Decoder(output_size=5, hidden_dim=5).to(device)\n",
        "batch_size = 16\n",
        "\n",
        "# Initial hidden state will be encoder's final hidden state\n",
        "decoder.init_hidden(hidden_final)\n",
        "# Initial data point is the last element of source sequence\n",
        "#inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)  # remove me\n",
        "\n",
        "decoder_outputs = []\n",
        "target_len = 2\n",
        "for i in range(target_len):\n",
        "    print(f'Hidden: {decoder.hidden}')\n",
        "    decoder_output = decoder(inputs)   # Predicts coordinates\n",
        "    decoder_outputs.append(decoder_output)\n",
        "    _, topi = decoder_output.topk(1)\n",
        "    inputs = topi.squeeze(-1).detach() # detach from history as input\n",
        "    print(f'Output: {decoder_output}\\n')\n",
        "decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "print(f'combinet_outputs: {decoder_outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gF2QNOHQFTKs"
      },
      "source": [
        "## Decoder with attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I1ibxeceU3OR"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim, input_dim=None, proj_values=False):\n",
        "        super().__init__()\n",
        "        self.d_k = hidden_dim\n",
        "        self.input_dim = hidden_dim if input_dim is None else input_dim\n",
        "        self.proj_values = proj_values\n",
        "        # Affine transformations for Q, K, and V\n",
        "        self.linear_query = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.linear_key = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.linear_value = nn.Linear(self.input_dim, hidden_dim)\n",
        "        self.alphas = None\n",
        "\n",
        "    def init_keys(self, keys):\n",
        "        self.keys = keys\n",
        "        self.proj_keys = self.linear_key(self.keys) # N, F, H x N, H, H -> N, F, H\n",
        "        self.values = self.linear_value(self.keys) if self.proj_values else self.keys  # N, F, H x N, H, H -> N, F, H\n",
        "\n",
        "    def score_function(self, query):\n",
        "        proj_query = self.linear_query(query) # N, 1, H x N, H, H -> N, 1, H\n",
        "        # scaled dot product\n",
        "        # N, 1, H x N, H, F -> N, 1, F\n",
        "        dot_products = torch.bmm(proj_query, self.proj_keys.permute(0, 2, 1))\n",
        "        scores =  dot_products / np.sqrt(self.d_k)\n",
        "        return scores\n",
        "\n",
        "    def forward(self, query, mask=None):\n",
        "        # Query is batch-first N, 1, H\n",
        "        # L or F means sequence length\n",
        "        scores = self.score_function(query) # N, 1, F\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alphas = F.softmax(scores, dim=-1) # N, 1, F\n",
        "        self.alphas = alphas.detach()\n",
        "\n",
        "        # N, 1, F x N, F, H -> N, 1, H\n",
        "        context = torch.bmm(alphas, self.values)\n",
        "        return context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4jhEhcL22Ze"
      },
      "source": [
        "### Decoder with rnn and attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XqGzrKwyb79a"
      },
      "outputs": [],
      "source": [
        "class DecoderAttn(nn.Module):\n",
        "    def __init__(self, output_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.hidden = None\n",
        "        self.output_size = output_size\n",
        "        self.embedding = nn.Embedding(output_size, self.hidden_dim)\n",
        "        self.basic_rnn = nn.GRU(self.hidden_dim, self.hidden_dim, batch_first=True)\n",
        "        self.attn = Attention(self.hidden_dim)\n",
        "        self.regression = nn.Linear(2 * self.hidden_dim, self.output_size)\n",
        "\n",
        "    def init_hidden(self, hidden_seq):\n",
        "        # the output of the encoder is N, F, H\n",
        "        # and init_keys expects batch-first as well\n",
        "        self.attn.init_keys(hidden_seq)\n",
        "        hidden_final = hidden_seq[:, -1:]\n",
        "        self.hidden = hidden_final.permute(1, 0, 2)   # F, N, H\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        # X is N, 1\n",
        "        # N is batch size, H is hidden dimensions\n",
        "        X = self.embedding(X) # N, 1 -> N, 1, H\n",
        "        batch_first_output, self.hidden = self.basic_rnn(X, self.hidden) # N, 1, H x N, H, H -> N, 1, H\n",
        "\n",
        "        query = batch_first_output # N, 1, H\n",
        "        # Attention\n",
        "        context = self.attn(query, mask=mask) # N, 1, H\n",
        "        concatenated = torch.cat([context, query], axis=-1) # N, 1, 2*H\n",
        "        out = self.regression(concatenated)  # N, 1, 2*H x N, 2*H, 1 -> N, 1, 1\n",
        "\n",
        "        # N, 1, F\n",
        "        return out.view(-1, 1, self.output_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmGWshzmwVMA"
      },
      "source": [
        "### Testing Decoder with attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PzV9yuD0r6SF"
      },
      "outputs": [],
      "source": [
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)  # remove me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5i1Ed221ruvx"
      },
      "outputs": [],
      "source": [
        "embedding = nn.Embedding(1, 5).to(device)\n",
        "embedding(inputs).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6s6X8wPInasL"
      },
      "outputs": [],
      "source": [
        "#torch.manual_seed(21)\n",
        "decoder = DecoderAttn(output_size=5, hidden_dim=5)\n",
        "\n",
        "# Initial hidden state will be encoder's final hidden state\n",
        "decoder.init_hidden(hidden_seq)\n",
        "# Initial data point is the last element of source sequence\n",
        "#inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)  # remove me\n",
        "\n",
        "decoder_outputs = []\n",
        "target_len = 2\n",
        "for i in range(target_len):\n",
        "    print(f'Hidden: {decoder.hidden}')\n",
        "    decoder_output = decoder(inputs)   # Predicts coordinates\n",
        "    decoder_outputs.append(decoder_output)\n",
        "    _, topi = decoder_output.topk(1)\n",
        "    inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "    print(f'Output: {decoder_output}\\n')\n",
        "decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "print(f'combinet_outputs: {decoder_outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8eUVUI1gbCw"
      },
      "source": [
        "## Encoder-Decoder Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrxSffbEvNUl"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_len, teacher_forcing_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_len = target_len\n",
        "        self.teacher_forcing_prob = teacher_forcing_prob\n",
        "        self.outputs = None\n",
        "\n",
        "    def init_outputs(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        # N, L (target), F\n",
        "        self.outputs = torch.zeros(batch_size,\n",
        "                              self.target_len,\n",
        "                              self.encoder.n_features).to(device)\n",
        "\n",
        "    def store_output(self, i, out):\n",
        "        # Stores the output\n",
        "        self.outputs[:, i:i+1, :] = out\n",
        "\n",
        "    def forward(self, X, target_tensor=None):\n",
        "        # X is batch of sentences -> N, F\n",
        "        # splits the data in source and target sequences\n",
        "        # the target seq will be empty in testing mode\n",
        "        # N, L, F\n",
        "\n",
        "        # Encoder expected N, F\n",
        "        hidden_seq, hidden_final = self.encoder(X)\n",
        "        # Output is N, F, hidden_dim\n",
        "        self.decoder.init_hidden(hidden_seq)\n",
        "\n",
        "        # The last input of the encoder is also\n",
        "        # the first input of the decoder\n",
        "        #dec_inputs = source_seq[:, -1:, :]\n",
        "        batch_size = hidden_seq.size(0)\n",
        "\n",
        "        dec_inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_outputs = []\n",
        "        # Generates as many outputs as the target length\n",
        "        for i in range(self.target_len):\n",
        "            # Output of decoder is N, 1, F\n",
        "            decoder_output = self.decoder(dec_inputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            prob = self.teacher_forcing_prob\n",
        "\n",
        "            # In evaluation/test the target sequence is\n",
        "            # unknown, so we cannot use teacher forcing\n",
        "            if not self.training:\n",
        "                prob = 0\n",
        "\n",
        "            if torch.rand(1) <= prob and target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "            #_, topi = decoder_output.topk(1)\n",
        "            #dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ_DI8_QG9yX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Testing encoder-decoder with attention\n",
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = DecoderAttn(output_lang.n_words, hidden_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R82lWKiwHNbx"
      },
      "outputs": [],
      "source": [
        "hidden_seq, hidden_final = encoder(full_seq)\n",
        "hidden_final.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1dmOyMYJHT6h"
      },
      "outputs": [],
      "source": [
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "decoder.init_hidden(hidden_seq)\n",
        "decoder(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U6fv9bBPKz4B"
      },
      "outputs": [],
      "source": [
        "encdec = EncoderDecoder(encoder, decoder, target_len=5)\n",
        "outputs = encdec(full_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hzf9lgD8vcVH"
      },
      "outputs": [],
      "source": [
        "outputs.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSk-uCP3mwCQ"
      },
      "source": [
        "## Encoder-Decoder Architecture With preprocessed embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH2EHzIbmwCX"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_len, teacher_forcing_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_len = target_len\n",
        "        self.teacher_forcing_prob = teacher_forcing_prob\n",
        "        self.outputs = None\n",
        "\n",
        "    def init_outputs(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        # N, L (target), F\n",
        "        self.outputs = torch.zeros(batch_size,\n",
        "                              self.target_len,\n",
        "                              self.encoder.n_features).to(device)\n",
        "\n",
        "    def store_output(self, i, out):\n",
        "        # Stores the output\n",
        "        self.outputs[:, i:i+1, :] = out\n",
        "\n",
        "    def forward(self, X, target_tensor=None):\n",
        "        # X is batch of sentences -> N, F\n",
        "        # splits the data in source and target sequences\n",
        "        # the target seq will be empty in testing mode\n",
        "        # N, L, F\n",
        "\n",
        "        # Encoder expected N, F\n",
        "        hidden_seq, hidden_final = self.encoder(X)\n",
        "        # Output is N, F, hidden_dim\n",
        "        self.decoder.init_hidden(hidden_seq)\n",
        "\n",
        "        # The last input of the encoder is also\n",
        "        # the first input of the decoder\n",
        "        #dec_inputs = source_seq[:, -1:, :]\n",
        "        batch_size = hidden_seq.size(0)\n",
        "\n",
        "        dec_inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_outputs = []\n",
        "        # Generates as many outputs as the target length\n",
        "        for i in range(self.target_len):\n",
        "            # Output of decoder is N, 1, F\n",
        "            decoder_output = self.decoder(dec_inputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            prob = self.teacher_forcing_prob\n",
        "\n",
        "            # In evaluation/test the target sequence is\n",
        "            # unknown, so we cannot use teacher forcing\n",
        "            if not self.training:\n",
        "                prob = 0\n",
        "\n",
        "            if torch.rand(1) <= prob and target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "            #_, topi = decoder_output.topk(1)\n",
        "            #dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing simple encoder-decoder with preprocessed embeddings\n",
        "hidden_size = 128\n",
        "encoder = Encoder(context_embeddings_tensor, hidden_dim=hidden_size).to(device)\n",
        "decoder = Decoder(pretrained_embeddings=context_embeddings_tensor, output_size=output_lang.n_words, hidden_dim=hidden_size).to(device)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Bz6NflvFmwCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "U-qxET5CmwCb"
      },
      "outputs": [],
      "source": [
        "#@title Testing encoder-decoder with attention\n",
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = DecoderAttn(output_lang.n_words, hidden_size).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AfmESVqemwCc"
      },
      "outputs": [],
      "source": [
        "hidden_seq, hidden_final = encoder(full_seq)\n",
        "hidden_final.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H5y7AJ7mwCd"
      },
      "outputs": [],
      "source": [
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "decoder.init_hidden(hidden_seq)\n",
        "decoder(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEcDlw_tmwCe"
      },
      "outputs": [],
      "source": [
        "encdec = EncoderDecoder(encoder, decoder, target_len=5)\n",
        "outputs = encdec(full_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDCNbC0XmwCe"
      },
      "outputs": [],
      "source": [
        "outputs.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoder Decoder using preprocessed embeddings"
      ],
      "metadata": {
        "id": "CherGzy1mWNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoder using preprocessed embeddings"
      ],
      "metadata": {
        "id": "_IfCbJq7DMbY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u1L37I5DUmO"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, pretrained_embeddings, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embd_dim = pretrained_embeddings.size(-1)\n",
        "        #self.n_features = n_features\n",
        "        self.hidden = None\n",
        "        #self.embedding_dimension = 256\n",
        "        #self.embd = nn.Embedding(self.n_features, self.hidden_dim)\n",
        "\n",
        "        # Create an Embedding layer and initialize it with the reshaped embeddings\n",
        "        self.embd = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "        self.basic_rnn = nn.GRU(self.embd_dim, self.hidden_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X_embd = self.embd(X) # N, F -> N, F, embd_dim\n",
        "        rnn_out, self.hidden = self.basic_rnn(X_embd) # N, F, embd_dim x N, embd_dim, H  ->  N, F, H\n",
        "\n",
        "        return rnn_out, self.hidden # N, F, H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhvRElwVDUmO"
      },
      "outputs": [],
      "source": [
        "embd = nn.Embedding(5, 5)\n",
        "rnn = nn.GRU(5, 5, batch_first=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1HUoMuxHDUmQ"
      },
      "outputs": [],
      "source": [
        "full_seq = torch.full((2, 3), 1)\n",
        "rnn_out, final_hidden = rnn(embd(full_seq))\n",
        "final_hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQpOv6AaDUmS"
      },
      "source": [
        "### Testig Encoder with preprocessed embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjMGvAtEDUmU"
      },
      "outputs": [],
      "source": [
        "#torch.manual_seed(21)\n",
        "full_seq = torch.full((2, 3), 1).to(device)\n",
        "encoder = Encoder(context_embeddings_tensor, hidden_dim=5).to(device)\n",
        "hidden_seq, hidden_final = encoder(full_seq) # output is N, L, F\n",
        "hidden_final"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple Decoder with preprocessed embeddings"
      ],
      "metadata": {
        "id": "1k0mF8zoFYHD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS-c_ZHkFeMs"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, pretrained_embeddings, output_size, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embd_dim = pretrained_embeddings.size(-1)\n",
        "        self.hidden = None\n",
        "        #self.embedding = nn.Embedding(output_size, self.hidden_dim)\n",
        "\n",
        "        self.embd = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "        self.basic_rnn = nn.GRU(self.embd_dim, self.hidden_dim, batch_first=True)\n",
        "        self.regression = nn.Linear(self.hidden_dim, output_size)\n",
        "\n",
        "    def init_hidden(self, hidden_final):\n",
        "        # We only need the final hidden state from encoder for each sentence\n",
        "        #hidden_final = hidden_seq[:, -1:] # N, F\n",
        "        #self.hidden = hidden_final\n",
        "        #self.hidden = hidden_final.permute(1, 0, 2) # 1, N, H\n",
        "\n",
        "        hidden_final = hidden_seq[:, -1:]\n",
        "        self.hidden = hidden_final.permute(1, 0, 2)   # F, N, H\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = self.embd(X) # N, 1 -> N, 1, embd_dim\n",
        "        batch_first_output, self.hidden = self.basic_rnn(X, self.hidden)  # N, 1, embd_dim x N, embd_dim, H -> N, 1, H\n",
        "        out = self.regression(batch_first_output) # N, 1, output_size\n",
        "\n",
        "        # N, 1, output_size\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uszb_sxlFeMu"
      },
      "source": [
        "### Testing Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JevclgNxFeMx"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(21)\n",
        "decoder = Decoder(pretrained_embeddings=context_embeddings_tensor, output_size=5, hidden_dim=5).to(device)\n",
        "batch_size = 16\n",
        "\n",
        "# Initial hidden state will be encoder's final hidden state\n",
        "decoder.init_hidden(hidden_seq)\n",
        "# Initial data point is the last element of source sequence\n",
        "#inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)  # remove me\n",
        "\n",
        "decoder_outputs = []\n",
        "target_len = 2\n",
        "for i in range(target_len):\n",
        "    print(f'Hidden: {decoder.hidden}')\n",
        "    decoder_output = decoder(inputs)   # Predicts coordinates\n",
        "    decoder_outputs.append(decoder_output)\n",
        "    _, topi = decoder_output.topk(1)\n",
        "    inputs = topi.squeeze(-1).detach() # detach from history as input\n",
        "    print(f'Output: {decoder_output}\\n')\n",
        "decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "print(f'combinet_outputs: {decoder_outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD2sb9nioxVy"
      },
      "source": [
        "## Encoder-Decoder Architecture With preprocessed embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zpr_xRBSoxVz"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_len, teacher_forcing_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_len = target_len\n",
        "        self.teacher_forcing_prob = teacher_forcing_prob\n",
        "        self.outputs = None\n",
        "\n",
        "    def init_outputs(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        # N, L (target), F\n",
        "        self.outputs = torch.zeros(batch_size,\n",
        "                              self.target_len,\n",
        "                              self.encoder.n_features).to(device)\n",
        "\n",
        "    def store_output(self, i, out):\n",
        "        # Stores the output\n",
        "        self.outputs[:, i:i+1, :] = out\n",
        "\n",
        "    def forward(self, X, target_tensor=None):\n",
        "        # X is batch of sentences -> N, F\n",
        "        # splits the data in source and target sequences\n",
        "        # the target seq will be empty in testing mode\n",
        "        # N, L, F\n",
        "\n",
        "        # Encoder expected N, F\n",
        "        hidden_seq, hidden_final = self.encoder(X)\n",
        "        # Output is N, F, hidden_dim\n",
        "        self.decoder.init_hidden(hidden_seq)\n",
        "\n",
        "        # The last input of the encoder is also\n",
        "        # the first input of the decoder\n",
        "        #dec_inputs = source_seq[:, -1:, :]\n",
        "        batch_size = hidden_seq.size(0)\n",
        "\n",
        "        dec_inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_outputs = []\n",
        "        # Generates as many outputs as the target length\n",
        "        for i in range(self.target_len):\n",
        "            # Output of decoder is N, 1, F\n",
        "            decoder_output = self.decoder(dec_inputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            prob = self.teacher_forcing_prob\n",
        "\n",
        "            # In evaluation/test the target sequence is\n",
        "            # unknown, so we cannot use teacher forcing\n",
        "            if not self.training:\n",
        "                prob = 0\n",
        "\n",
        "            if torch.rand(1) <= prob and target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                decoder_input = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
        "            #_, topi = decoder_output.topk(1)\n",
        "            #dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Testing simple encoder-decoder with preprocessed embeddings\n",
        "hidden_size = 128\n",
        "encoder = Encoder(context_embeddings_tensor, hidden_dim=hidden_size).to(device)\n",
        "decoder = Decoder(pretrained_embeddings=context_embeddings_tensor, output_size=output_lang.n_words, hidden_dim=hidden_size).to(device)"
      ],
      "metadata": {
        "id": "FP8FeZSvoxV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YdSzTyToxV4"
      },
      "outputs": [],
      "source": [
        "hidden_seq, hidden_final = encoder(full_seq)\n",
        "hidden_final.size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7tbtHTV8oxV6"
      },
      "outputs": [],
      "source": [
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "decoder.init_hidden(hidden_seq)\n",
        "decoder(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DvVrGjF1oxV7"
      },
      "outputs": [],
      "source": [
        "encdec = EncoderDecoder(encoder, decoder, target_len=5)\n",
        "outputs = encdec(full_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXD2WBGtoxV9"
      },
      "outputs": [],
      "source": [
        "outputs.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dylJvhIga3Ef"
      },
      "source": [
        "# Encoder-Decoder with Self Attention (Most Advanced Model) (WITHOUT RNN)\n",
        "It is not working, and is giving me repetitive predictions.\n",
        "---   \n",
        "**_I pity the fool using RNN_**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVFvcI2-ofBI"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, input_dim=None, proj_values=True):\n",
        "        super().__init__()\n",
        "        self.linear_out = nn.Linear(n_heads * d_model, d_model)\n",
        "        self.attn_heads = nn.ModuleList([Attention(d_model,\n",
        "                                                   input_dim=input_dim,\n",
        "                                                   proj_values=proj_values)\n",
        "                                         for _ in range(n_heads)])\n",
        "\n",
        "    def init_keys(self, key):\n",
        "        for attn in self.attn_heads:\n",
        "            attn.init_keys(key)\n",
        "\n",
        "    @property\n",
        "    def alphas(self):\n",
        "        # Shape: n_heads, N, 1, L (source)\n",
        "        return torch.stack([attn.alphas for attn in self.attn_heads], dim=0)\n",
        "\n",
        "    def output_function(self, contexts):\n",
        "        # N, 1, n_heads * D\n",
        "        concatenated = torch.cat(contexts, axis=-1)\n",
        "        # Linear transf. to go back to original dimension\n",
        "        out = self.linear_out(concatenated) # N, 1, D\n",
        "        return out\n",
        "\n",
        "    def forward(self, query, mask=None):\n",
        "        contexts = [attn(query, mask=mask) for attn in self.attn_heads]\n",
        "        out = self.output_function(contexts)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjgUkH4J2wL-"
      },
      "source": [
        "### Encoder with self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "er7YU4JoqQbG"
      },
      "outputs": [],
      "source": [
        "class EncoderSelfAttn(nn.Module):\n",
        "    def __init__(self, n_features, d_model, n_heads, ff_units):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model  # d_model is just another name for hidden_dim\n",
        "        self.ff_units = ff_units\n",
        "        self.n_features = n_features\n",
        "        self.n_heads = n_heads\n",
        "        self.hidden = None\n",
        "        self.embd = nn.Embedding(self.n_features, self.d_model)\n",
        "        self.self_attn_heads = MultiHeadAttention(n_heads, d_model, input_dim=self.d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_units, d_model),\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        # X is source sequence of shape N, F\n",
        "        X_embd = self.embd(X) # N, F -> N, F, H\n",
        "        #rnn_out, self.hidden = self.basic_rnn(X_embd) # N, F, H x N, H, H  ->  N, F, H\n",
        "\n",
        "        self.self_attn_heads.init_keys(X_embd)\n",
        "        att = self.self_attn_heads(X_embd) # N, F, n_heads * H\n",
        "        out = self.ffn(att)  # N, F, H\n",
        "\n",
        "        return out # N, F, H"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFr44kMRypLC"
      },
      "outputs": [],
      "source": [
        "inputs = torch.empty(2, 2, dtype=torch.long, device=device).fill_(SOS_token)  # remove me"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TrGhk2KygQv"
      },
      "outputs": [],
      "source": [
        "encoderSelfAttention = EncoderSelfAttn(n_features=2, d_model=5, n_heads=4, ff_units=4).to(device)\n",
        "hidden_seq = encoderSelfAttention(inputs)\n",
        "hidden_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COwOMyr93GoU"
      },
      "source": [
        "### Decoder with self attention and cross attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWFVw23S3M5I"
      },
      "outputs": [],
      "source": [
        "class DecoderSelfAttn(nn.Module):\n",
        "    def __init__(self, n_features, d_model, n_heads, ff_units):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.hidden = None\n",
        "        self.n_features = n_features\n",
        "        self.embedding = nn.Embedding(self.n_features, self.d_model)\n",
        "        self.self_attn_heads = MultiHeadAttention(n_heads, self.d_model, input_dim=self.d_model)\n",
        "        self.cross_attn_heads = MultiHeadAttention(n_heads, self.d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(2*d_model, ff_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ff_units, self.n_features),\n",
        "        )\n",
        "\n",
        "    def init_hidden(self, hidden_seq):\n",
        "        # the output of the encoder is N, F, H\n",
        "        # and init_keys expects batch-first as well\n",
        "        self.cross_attn_heads.init_keys(hidden_seq)\n",
        "\n",
        "    def forward(self, X, mask=None):\n",
        "        # X is N, 1\n",
        "        # N is batch size, H is hidden dimensions\n",
        "        X_embd = self.embedding(X) # N, 1 -> N, 1, H\n",
        "        self.self_attn_heads.init_keys(X_embd)\n",
        "        attn = self.self_attn_heads(X_embd)  # N, 1, H\n",
        "        attn2 = self.cross_attn_heads(attn)  # N, 1, H\n",
        "\n",
        "        concatenated = torch.cat([attn2, X_embd], axis=-1) # N, 1, 2*H\n",
        "        #out = self.regression(concatenated)  # N, 1, 2*H x N, 2*H, 1 -> N, 1, 1\n",
        "        out = self.ffn(concatenated) # N, 1, n_features\n",
        "\n",
        "        # N, 1, F\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sx1z63096l2t"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(21)\n",
        "decoder = DecoderSelfAttn(n_features=5, d_model=5, n_heads=5, ff_units=5)\n",
        "\n",
        "# Initial hidden state will be encoder's final hidden state\n",
        "decoder.init_hidden(hidden_seq)\n",
        "# Initial data point is the last element of source sequence\n",
        "#inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "inputs = torch.empty(2, 1, dtype=torch.long, device=device).fill_(SOS_token)  # remove me\n",
        "\n",
        "decoder_outputs = []\n",
        "target_len = 2\n",
        "for i in range(target_len):\n",
        "    decoder_output = decoder(inputs)   # Predicts coordinates\n",
        "    decoder_outputs.append(decoder_output)\n",
        "    _, topi = decoder_output.topk(1)\n",
        "    inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "    print(f'Output: {decoder_output}\\n')\n",
        "decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "print(f'combinet_outputs: {decoder_outputs}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO4ZwLld-oR8"
      },
      "source": [
        "## Encoder-Decoder Self-Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-up-_8Jt94Kt"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderSelfAttention(nn.Module):\n",
        "    def __init__(self, encoder, decoder, target_len, teacher_forcing_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_len = target_len\n",
        "        self.teacher_forcing_prob = teacher_forcing_prob\n",
        "        self.outputs = None\n",
        "\n",
        "    def init_outputs(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        # N, L (target), F\n",
        "        self.outputs = torch.zeros(batch_size,\n",
        "                              self.target_len,\n",
        "                              self.encoder.n_features).to(device)\n",
        "\n",
        "    def store_output(self, i, out):\n",
        "        # Stores the output\n",
        "        self.outputs[:, i:i+1, :] = out\n",
        "\n",
        "    def forward(self, X, target_tensor=None):\n",
        "        # X is batch of sentences -> N, F\n",
        "        # splits the data in source and target sequences\n",
        "        # the target seq will be empty in testing mode\n",
        "        # N, L, F\n",
        "\n",
        "        # Encoder expected N, F\n",
        "        hidden_seq = self.encoder(X)\n",
        "        # Output is N, F, hidden_dim\n",
        "        self.decoder.init_hidden(hidden_seq)\n",
        "\n",
        "        # The last input of the encoder is also\n",
        "        # the first input of the decoder\n",
        "        #dec_inputs = source_seq[:, -1:, :]\n",
        "        batch_size = hidden_seq.size(0)\n",
        "\n",
        "        dec_inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_outputs = []\n",
        "        # Generates as many outputs as the target length\n",
        "        for i in range(self.target_len):\n",
        "            # Output of decoder is N, 1, F\n",
        "            decoder_output = self.decoder(dec_inputs)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            prob = self.teacher_forcing_prob\n",
        "\n",
        "            # In evaluation/test the target sequence is\n",
        "            # unknown, so we cannot use teacher forcing\n",
        "            if not self.training:\n",
        "                prob = 0\n",
        "\n",
        "            if torch.rand(1) <= prob and target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                dec_inputs = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "            #_, topi = decoder_output.topk(1)\n",
        "            #dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vhXkNfgLLwl"
      },
      "outputs": [],
      "source": [
        "full_seq = torch.full((2, 3), 1)\n",
        "full_seq = full_seq.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBCP3IDg9Nea"
      },
      "outputs": [],
      "source": [
        "hidden_size = 128\n",
        "batch_size = 32\n",
        "encoder = EncoderSelfAttn(n_features=input_lang.n_words, d_model=hidden_size, n_heads=4, ff_units=4).to(device)\n",
        "decoder = DecoderSelfAttn(n_features=output_lang.n_words, d_model=hidden_size, n_heads=4, ff_units=4).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO_Qae7P-Gb7"
      },
      "outputs": [],
      "source": [
        "encdec = EncoderDecoderSelfAttention(encoder, decoder, target_len=10)\n",
        "outputs = encdec(full_seq)\n",
        "outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxgdCVdtFblT"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRy7FoqhIb6b"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
        "        angular_speed = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * angular_speed) # even dimensions\n",
        "        pe[:, 1::2] = torch.cos(position * angular_speed) # odd dimensions\n",
        "        self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x is N, L, D\n",
        "        # pe is 1, maxlen, D\n",
        "        scaled_x = x * np.sqrt(self.d_model)\n",
        "        encoded = scaled_x + self.pe[:, :x.size(1), :]\n",
        "        return encoded # N, L, D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CUR65dSMhU4"
      },
      "outputs": [],
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, dropout=0.1):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.d_k = int(d_model / n_heads)\n",
        "        self.linear_query = nn.Linear(d_model, d_model)\n",
        "        self.linear_key = nn.Linear(d_model, d_model)\n",
        "        self.linear_value = nn.Linear(d_model, d_model)\n",
        "        self.linear_out = nn.Linear(d_model, d_model)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.alphas = None\n",
        "\n",
        "    def make_chunks(self, x):\n",
        "        batch_size, seq_len = x.size(0), x.size(1)\n",
        "        # N, L, D -> N, L, n_heads * d_k\n",
        "        x = x.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "        # N, n_heads, L, d_k\n",
        "        x = x.transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "    def init_keys(self, key):\n",
        "        # N, n_heads, L, d_k\n",
        "        self.proj_key = self.make_chunks(self.linear_key(key))\n",
        "        self.proj_value = self.make_chunks(self.linear_value(key))\n",
        "\n",
        "    def score_function(self, query):\n",
        "        # scaled dot product\n",
        "        # N, n_heads, L, d_k x # N, n_heads, d_k, L -> N, n_heads, L, L\n",
        "        proj_query = self.make_chunks(self.linear_query(query))\n",
        "        dot_products = torch.matmul(proj_query,\n",
        "                                    self.proj_key.transpose(-2, -1))\n",
        "        scores =  dot_products / np.sqrt(self.d_k)\n",
        "        return scores\n",
        "\n",
        "    def attn(self, query, mask=None):\n",
        "        # Query is batch-first: N, L, D\n",
        "        # Score function will generate scores for each head\n",
        "        scores = self.score_function(query) # N, n_heads, L, L\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        alphas = F.softmax(scores, dim=-1) # N, n_heads, L, L\n",
        "        alphas = self.dropout(alphas)\n",
        "        self.alphas = alphas.detach()\n",
        "\n",
        "        # N, n_heads, L, L x N, n_heads, L, d_k -> N, n_heads, L, d_k\n",
        "        context = torch.matmul(alphas, self.proj_value)\n",
        "        return context\n",
        "\n",
        "    def output_function(self, contexts):\n",
        "        # N, L, D\n",
        "        out = self.linear_out(contexts) # N, L, D\n",
        "        return out\n",
        "\n",
        "    def forward(self, query, mask=None):\n",
        "        if mask is not None:\n",
        "            # N, 1, L, L - every head uses the same mask\n",
        "            mask = mask.unsqueeze(1)\n",
        "\n",
        "        # N, n_heads, L, d_k\n",
        "        context = self.attn(query, mask=mask)\n",
        "        # N, L, n_heads, d_k\n",
        "        context = context.transpose(1, 2).contiguous()\n",
        "        # N, L, n_heads * d_k = N, L, d_model\n",
        "        context = context.view(query.size(0), -1, self.d_model)\n",
        "        # N, L, d_model\n",
        "        out = self.output_function(context)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "watch1 = \"\"\"\n",
        "The Hatter was the first to break the silence. `What day of the month is it?' he said, turning to Alice:  he had taken his watch out of his pocket, and was looking at it uneasily, shaking it every now and then, and holding it to his ear.\n",
        "\"\"\"\n",
        "\n",
        "watch2 = \"\"\"\n",
        "Alice thought this a very curious thing, and she went nearer to watch them, and just as she came up to them she heard one of them say, `Look out now, Five!  Don't go splashing paint over me like that!\n",
        "\"\"\"\n",
        "\n",
        "sentences = [watch1, watch2]"
      ],
      "metadata": {
        "id": "8T6TPZnSU9QJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp3LpDtwPIph"
      },
      "source": [
        "## Preprocessing Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Ke5z7WNwT1"
      },
      "source": [
        "## Transformer Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXYoPU3UNwT2"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/enc_both.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zVPRvS0tNwT2"
      },
      "source": [
        "$$\n",
        "\\large\n",
        "\\begin{aligned}\n",
        "&\\text{outputs}_{\\text{norm-last}}=&\\text{norm}(\\underbrace{\\text{norm(inputs + att(inputs))}}_{\\text{Output of SubLayer}_0} + \\text{ffn}(\\underbrace{\\text{norm(inputs + att(inputs))}}_{\\text{Output of SubLayer}_0}))\n",
        "\\\\\n",
        "\\\\\n",
        "&\\text{outputs}_{\\text{norm-first}}=&\\underbrace{\\text{inputs + att(norm(inputs))}}_{\\text{Output of SubLayer}_0}+\\text{ffn(norm(}\\underbrace{\\text{inputs + att(norm(inputs))}}_{\\text{Output of SubLayer}_0}))\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYdXRKjbNwT3"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.ff_units = ff_units\n",
        "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
        "                                                    dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_units, d_model),\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, mask=None):\n",
        "        # Sublayer #0\n",
        "        # Norm\n",
        "        norm_query = self.norm1(query)\n",
        "        # Multi-headed Attention\n",
        "        self.self_attn_heads.init_keys(norm_query)\n",
        "        states = self.self_attn_heads(norm_query, mask) # N, L, D\n",
        "        # Add\n",
        "        att = query + self.drop1(states) # N, L, D\n",
        "\n",
        "        # Sublayer #1\n",
        "        # Norm\n",
        "        norm_att = self.norm2(att) # N, L, D\n",
        "        # Feed Forward\n",
        "        out = self.ffn(norm_att) # N, L, D\n",
        "        # Add\n",
        "        out = att + self.drop2(out)\n",
        "        return out # N, L, D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ex52QCuNwT3"
      },
      "outputs": [],
      "source": [
        "class EncoderTransf(nn.Module):\n",
        "    def __init__(self, encoder_layer, n_features, n_layers=1, max_len=100):\n",
        "        super().__init__()\n",
        "        self.d_model = encoder_layer.d_model\n",
        "        self.n_features = n_features\n",
        "        self.embd = nn.Embedding(self.n_features, self.d_model)\n",
        "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
        "        self.norm = nn.LayerNorm(self.d_model)\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "    def forward(self, query, mask=None):\n",
        "        # query is source sequence of shape N, L    i.e. N is the batch size, L is the sequence length\n",
        "        X_embd = self.embd(query) # N, L -> N, L, D\n",
        "        # Positional Encoding\n",
        "        x = self.pe(X_embd) # N, L, D\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask) # N, L, D\n",
        "        # Norm\n",
        "        return self.norm(x) # N, L, D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoder transformer with preprocessed bert embeddings (Ignore for now)"
      ],
      "metadata": {
        "id": "wslkM5DcTTzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_embeddings_tensor.size()"
      ],
      "metadata": {
        "id": "SpztgQXsesK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class MultiHeadedAttention(nn.Module):\n",
        "#     def __init__(self, n_heads, d_ff, d_model, dropout=0.1):\n",
        "#         super(MultiHeadedAttention, self).__init__()\n",
        "#         self.n_heads = n_heads\n",
        "#         self.d_model = d_model\n",
        "#         self.d_ff = d_ff\n",
        "#         self.d_k = int(d_model / n_heads)\n",
        "#         self.linear_query = nn.Linear(d_model, d_model)\n",
        "#         self.linear_key = nn.Linear(d_model, d_model)\n",
        "#         self.linear_value = nn.Linear(d_model, d_model)\n",
        "#         self.linear_out = nn.Linear(d_model, d_ff)\n",
        "#         self.dropout = nn.Dropout(p=dropout)\n",
        "#         self.alphas = None\n",
        "\n",
        "#     def make_chunks(self, x):\n",
        "#         batch_size, seq_len = x.size(0), x.size(1)\n",
        "#         # N, L, D -> N, L, n_heads * d_k\n",
        "#         x = x.view(batch_size, seq_len, self.n_heads, self.d_k)\n",
        "#         # N, n_heads, L, d_k\n",
        "#         x = x.transpose(1, 2)\n",
        "#         return x\n",
        "\n",
        "#     def init_keys(self, key):\n",
        "#         # N, n_heads, L, d_k\n",
        "#         self.proj_key = self.make_chunks(self.linear_key(key))\n",
        "#         self.proj_value = self.make_chunks(self.linear_value(key))\n",
        "\n",
        "#     def score_function(self, query):\n",
        "#         # scaled dot product\n",
        "#         # N, n_heads, L, d_k x # N, n_heads, d_k, L -> N, n_heads, L, L\n",
        "#         proj_query = self.make_chunks(self.linear_query(query))\n",
        "#         dot_products = torch.matmul(proj_query,\n",
        "#                                     self.proj_key.transpose(-2, -1))\n",
        "#         scores =  dot_products / np.sqrt(self.d_k)\n",
        "#         return scores\n",
        "\n",
        "#     def attn(self, query, mask=None):\n",
        "#         # Query is batch-first: N, L, D\n",
        "#         # Score function will generate scores for each head\n",
        "#         scores = self.score_function(query) # N, n_heads, L, L\n",
        "#         if mask is not None:\n",
        "#             scores = scores.masked_fill(mask == 0, -1e9)\n",
        "#         alphas = F.softmax(scores, dim=-1) # N, n_heads, L, L\n",
        "#         alphas = self.dropout(alphas)\n",
        "#         self.alphas = alphas.detach()\n",
        "\n",
        "#         # N, n_heads, L, L x N, n_heads, L, d_k -> N, n_heads, L, d_k\n",
        "#         context = torch.matmul(alphas, self.proj_value)\n",
        "#         return context\n",
        "\n",
        "#     def output_function(self, contexts):\n",
        "#         # N, L, D\n",
        "#         out = self.linear_out(contexts) # N, L, d_ff\n",
        "#         return out\n",
        "\n",
        "#     def forward(self, query, mask=None):\n",
        "#         if mask is not None:\n",
        "#             # N, 1, L, L - every head uses the same mask\n",
        "#             mask = mask.unsqueeze(1)\n",
        "\n",
        "#         # N, n_heads, L, d_k\n",
        "#         context = self.attn(query, mask=mask)\n",
        "#         # N, L, n_heads, d_k\n",
        "#         context = context.transpose(1, 2).contiguous()\n",
        "#         # N, L, n_heads * d_k = N, L, d_model\n",
        "#         context = context.view(query.size(0), -1, self.d_model)\n",
        "#         # N, L, d_ff\n",
        "#         out = self.output_function(context)\n",
        "#         return out # N, L, d_ff"
      ],
      "metadata": {
        "id": "i5vA4UV5jCaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class EncoderLayer(nn.Module):\n",
        "#     def __init__(self, n_heads, d_ff, d_model, ff_units, dropout=0.1):\n",
        "#         super().__init__()\n",
        "#         self.n_heads = n_heads\n",
        "#         self.d_ff = d_ff\n",
        "#         self.d_model = d_model\n",
        "#         self.ff_units = ff_units\n",
        "#         self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
        "#                                                     dropout=dropout)\n",
        "#         self.ffn = nn.Sequential(\n",
        "#             nn.Linear(d_model, ff_units),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(dropout),\n",
        "#             nn.Linear(ff_units, d_ff),\n",
        "#         )\n",
        "\n",
        "#         self.norm1 = nn.LayerNorm(d_model)\n",
        "#         self.norm2 = nn.LayerNorm(d_model)\n",
        "#         self.drop1 = nn.Dropout(dropout)\n",
        "#         self.drop2 = nn.Dropout(dropout)\n",
        "\n",
        "#     def forward(self, query, mask=None):\n",
        "#         # Sublayer #0\n",
        "#         # Norm\n",
        "#         norm_query = self.norm1(query)\n",
        "#         # Multi-headed Attention\n",
        "#         self.self_attn_heads.init_keys(norm_query)\n",
        "#         states = self.self_attn_heads(norm_query, mask) # N, L, d_ff\n",
        "#         # Add\n",
        "#         att = query + self.drop1(states) # N, L, d_ff\n",
        "\n",
        "#         # Sublayer #1\n",
        "#         # Norm\n",
        "#         norm_att = self.norm2(att) # N, L, D\n",
        "#         # Feed Forward\n",
        "#         out = self.ffn(norm_att) # N, L, d_ff\n",
        "#         # Add\n",
        "#         out = att + self.drop2(out)\n",
        "#         return out # N, L, d_ff"
      ],
      "metadata": {
        "id": "Ug4dPSMThiqA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class EncoderTransf(nn.Module):\n",
        "#     def __init__(self, encoder_layer, pretrained_embeddings, n_features, n_layers=1, max_len=100):\n",
        "#         super().__init__()\n",
        "#         self.d_model = encoder_layer.d_model\n",
        "#         self.embd_dim = pretrained_embeddings.size(-1)\n",
        "#         self.n_features = n_features\n",
        "#         #self.embd = nn.Embedding(self.n_features, self.d_model)\n",
        "#         # Reshape the embeddings to (Vocabulary Size, Embedding Dimension)\n",
        "\n",
        "#         # Create an Embedding layer and initialize it with the reshaped embeddings\n",
        "#         self.embd = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
        "#         self.pe = PositionalEncoding(max_len, self.d_model)\n",
        "#         self.norm = nn.LayerNorm(self.d_model)\n",
        "#         self.layers = nn.ModuleList([copy.deepcopy(encoder_layer)\n",
        "#                                      for _ in range(n_layers)])\n",
        "\n",
        "#     def forward(self, query, mask=None):\n",
        "#         # query is source sequence of shape N, L    i.e. N is the batch size, L is the sequence length\n",
        "#         X_embd = self.embd(query) # N, L -> N, L, D\n",
        "#         # Positional Encoding\n",
        "#         x = self.pe(X_embd) # N, L, D\n",
        "#         for layer in self.layers:\n",
        "#             x = layer(x, mask) # N, L, D\n",
        "#         # Norm\n",
        "#         return self.norm(x) # N, L, D"
      ],
      "metadata": {
        "id": "zoBD1SHtTTLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing Encoder with preprocessed embeddings"
      ],
      "metadata": {
        "id": "nFV9RTzk9WV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# enclayer = EncoderLayer(d_model=context_embeddings_tensor.size(-1), n_heads=3, ff_units=20)\n",
        "# enctransf = EncoderTransf(enclayer, pretrained_embeddings=context_embeddings_tensor, n_features=2, n_layers=1)"
      ],
      "metadata": {
        "id": "6mkutfOt86W6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iiJs_GcNkcb"
      },
      "outputs": [],
      "source": [
        "# full_seq = torch.full((2, 2), 1)  # remove me\n",
        "# outputs = enctransf(full_seq)\n",
        "# outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVTdyiMev4_6"
      },
      "source": [
        "### Testing Transformer Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JPBlyyiiNA0P"
      },
      "outputs": [],
      "source": [
        "enclayer = EncoderLayer(d_model=6, n_heads=3, ff_units=20)\n",
        "enctransf = EncoderTransf(enclayer, n_features=2, n_layers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYcNyRvgNwT4"
      },
      "outputs": [],
      "source": [
        "#enclayer = nn.TransformerEncoderLayer(d_model=6, nhead=3, dim_feedforward=20)\n",
        "#enctransf = nn.TransformerEncoder(enclayer, num_layers=1, norm=nn.LayerNorm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZHPrqBeNwT4"
      },
      "source": [
        "## Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMBfkO6KNwT5"
      },
      "source": [
        "![](https://github.com/dvgodoy/PyTorchStepByStep/blob/master/images/dec_both.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IihqH32DNwT5"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, n_heads, d_model, ff_units, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.d_model = d_model\n",
        "        self.ff_units = ff_units\n",
        "        self.self_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
        "                                                    dropout=dropout)\n",
        "        self.cross_attn_heads = MultiHeadedAttention(n_heads, d_model,\n",
        "                                                     dropout=dropout)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ff_units),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_units, d_model),\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.drop1 = nn.Dropout(dropout)\n",
        "        self.drop2 = nn.Dropout(dropout)\n",
        "        self.drop3 = nn.Dropout(dropout)\n",
        "\n",
        "    def init_keys(self, states):\n",
        "        self.cross_attn_heads.init_keys(states)\n",
        "\n",
        "    def forward(self, query, source_mask=None, target_mask=None):\n",
        "        # Sublayer #0\n",
        "        # Norm\n",
        "        norm_query = self.norm1(query)\n",
        "        # Masked Multi-head Attention\n",
        "        self.self_attn_heads.init_keys(norm_query)\n",
        "        states = self.self_attn_heads(norm_query, target_mask)   # N, 1, D\n",
        "        # Add\n",
        "        att1 = query + self.drop1(states)\n",
        "\n",
        "        # Sublayer #1\n",
        "        # Norm\n",
        "        norm_att1 = self.norm2(att1)\n",
        "        # Multi-head Attention\n",
        "        encoder_states = self.cross_attn_heads(norm_att1, source_mask) # N, 1, D\n",
        "        # Add\n",
        "        att2 = att1 + self.drop2(encoder_states)\n",
        "\n",
        "        # Sublayer #2\n",
        "        # Norm\n",
        "        norm_att2 = self.norm3(att2)\n",
        "        # Feed Forward\n",
        "        out = self.ffn(norm_att2) # N, 1, D\n",
        "        # Add\n",
        "        out = att2 + self.drop3(out)\n",
        "        return out # N, 1, D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dEtnxSOCNwT5"
      },
      "outputs": [],
      "source": [
        "class DecoderTransf(nn.Module):\n",
        "    def __init__(self, decoder_layer, output_features, n_layers=1, max_len=100):\n",
        "        super(DecoderTransf, self).__init__()\n",
        "        self.d_model = decoder_layer.d_model\n",
        "        self.embedding = nn.Embedding(output_features, self.d_model)\n",
        "        self.pe = PositionalEncoding(max_len, self.d_model)\n",
        "        self.norm = nn.LayerNorm(self.d_model)\n",
        "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer)\n",
        "                                     for _ in range(n_layers)])\n",
        "\n",
        "    def init_keys(self, states):\n",
        "        for layer in self.layers:\n",
        "            layer.init_keys(states)\n",
        "\n",
        "    def forward(self, query, source_mask=None, target_mask=None):\n",
        "        # Embedding\n",
        "        query = self.embedding(query) # N, 1 -> N, 1, D\n",
        "        # Positional Encoding\n",
        "        x = self.pe(query)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, source_mask, target_mask)\n",
        "        # Norm\n",
        "        return self.norm(x) # N, 1, D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMq8Svd3vnbs"
      },
      "source": [
        "## Transformer Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFB69cIbztyr"
      },
      "outputs": [],
      "source": [
        "class EncoderDecoderSelfAttentionTransformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder, output_features, target_len, teacher_forcing_prob=0.5):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.target_len = target_len\n",
        "        self.teacher_forcing_prob = teacher_forcing_prob\n",
        "        self.outputs = None\n",
        "        self.linear = nn.Linear(encoder.d_model, output_features)\n",
        "\n",
        "    def init_outputs(self, batch_size):\n",
        "        device = next(self.parameters()).device\n",
        "        # N, L (target), F\n",
        "        self.outputs = torch.zeros(batch_size,\n",
        "                              self.target_len,\n",
        "                              self.encoder.n_features).to(device)\n",
        "\n",
        "    def store_output(self, i, out):\n",
        "        # Stores the output\n",
        "        self.outputs[:, i:i+1, :] = out\n",
        "\n",
        "    def encode(self, source_seq, source_mask=None):\n",
        "        # Projection\n",
        "        #source_proj = self.proj(source_seq)\n",
        "        encoder_states = self.encoder(source_seq, source_mask)\n",
        "        self.decoder.init_keys(encoder_states)\n",
        "        return encoder_states\n",
        "\n",
        "    def decode(self, shifted_target_seq, source_mask=None, target_mask=None):\n",
        "        # Projection\n",
        "        #target_proj = self.proj(shifted_target_seq)\n",
        "        outputs = self.decoder(shifted_target_seq,\n",
        "                               source_mask=source_mask,\n",
        "                               target_mask=target_mask)\n",
        "        # Linear\n",
        "        outputs = self.linear(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def forward(self, X, target_tensor=None):\n",
        "        # X is batch of sentences -> N, L\n",
        "        # splits the data in source and target sequences\n",
        "        # the target seq will be empty in testing mode\n",
        "        # N, L, D\n",
        "\n",
        "        encoder_states = self.encode(X) # N, L, D\n",
        "        # Encoder expected N, L\n",
        "        #encoder_states = self.encoder(X)\n",
        "        # Output is N, L, D\n",
        "        #self.decoder.init_keys(encoder_states)\n",
        "\n",
        "        # The last input of the encoder is also\n",
        "        # the first input of the decoder\n",
        "        #dec_inputs = source_seq[:, -1:, :]\n",
        "        batch_size = encoder_states.size(0)\n",
        "\n",
        "        dec_inputs = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
        "        decoder_outputs = []\n",
        "        # Generates as many outputs as the target length\n",
        "        for i in range(self.target_len):\n",
        "            # Output of decoder is N, 1, D\n",
        "            decoder_output = self.decode(dec_inputs)\n",
        "            #decoder_output = self.linear(decoder_output)\n",
        "            decoder_outputs.append(decoder_output)\n",
        "\n",
        "            prob = self.teacher_forcing_prob\n",
        "\n",
        "            # In evaluation/test the target sequence is\n",
        "            # unknown, so we cannot use teacher forcing\n",
        "            if not self.training:\n",
        "                prob = 0\n",
        "\n",
        "            if torch.rand(1) <= prob and target_tensor is not None:\n",
        "                # Teacher forcing: Feed the target as the next input\n",
        "                dec_inputs = target_tensor[:, i].unsqueeze(1) # Teacher forcing\n",
        "            else:\n",
        "                # Without teacher forcing: use its own predictions as the next input\n",
        "                _, topi = decoder_output.topk(1)\n",
        "                dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "            #_, topi = decoder_output.topk(1)\n",
        "            #dec_inputs = topi.squeeze(-1).detach()  # detach from history as input\n",
        "\n",
        "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
        "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
        "        return decoder_outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvKOj8gSvt_W"
      },
      "source": [
        "### Testing Encoder-Decoder Transformer architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsfqpHvX5RaC"
      },
      "outputs": [],
      "source": [
        "full_seq = torch.full((2, 5), 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLGAEpLg5ndq"
      },
      "outputs": [],
      "source": [
        "enclayer = EncoderLayer(d_model=12, n_heads=3, ff_units=20)\n",
        "enctransf = EncoderTransf(enclayer, n_features=input_lang.n_words, n_layers=1)\n",
        "\n",
        "declayer = DecoderLayer(d_model=12, n_heads=3, ff_units=20)\n",
        "dectransf = DecoderTransf(decoder_layer=declayer, output_features=output_lang.n_words, n_layers=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYzpBmJc5yv7"
      },
      "outputs": [],
      "source": [
        "enc_dec = EncoderDecoderSelfAttentionTransformer(enctransf, dectransf, output_features=output_lang.n_words, target_len=2)\n",
        "enc_dec(full_seq).size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6bjz3ixK_HD"
      },
      "outputs": [],
      "source": [
        "# Encoder-Decoder with Attention and RNN\n",
        "#encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "#decoder = DecoderAttn(output_lang.n_words, hidden_size).to(device)\n",
        "#model = EncoderDecoder(encoder, decoder, target_len=10)\n",
        "# Transformer\n",
        "enclayer = EncoderLayer(d_model=hidden_size, n_heads=3, ff_units=20)\n",
        "enctransf = EncoderTransf(enclayer, n_features=input_lang.n_words, n_layers=1)\n",
        "declayer = DecoderLayer(d_model=hidden_size, n_heads=3, ff_units=20)\n",
        "dectransf = DecoderTransf(decoder_layer=declayer, output_features=output_lang.n_words, n_layers=1)\n",
        "model = EncoderDecoderSelfAttentionTransformer(enctransf, dectransf, output_features=output_lang.n_words, target_len=10)\n",
        "\n",
        "for data in train_dataloader:\n",
        "    input_tensor, target_tensor = data\n",
        "    decoder_outputs = model(input_tensor, target_tensor)\n",
        "    print(f\"target_tensor: {target_tensor.view(-1).size()}, decoder_outputs: {decoder_outputs.view(-1, decoder_outputs.size(-1)).size()}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C9R5Qe7gNJ-"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t3sXkZ6xNOWV"
      },
      "outputs": [],
      "source": [
        "def train_epoch(dataloader, model, optimizer, criterion):\n",
        "\n",
        "    total_loss = 0\n",
        "    for data in dataloader:\n",
        "        input_tensor, target_tensor = data\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        decoder_outputs = model(input_tensor, target_tensor)\n",
        "\n",
        "        loss = criterion(\n",
        "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
        "            target_tensor.view(-1)\n",
        "        )\n",
        "        loss.backward()\n",
        "\n",
        "        # Step 4 - Updates parameters using gradients and the learning rate\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU08RsKeNOWY"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQM1wNGaNOWc"
      },
      "outputs": [],
      "source": [
        "def train(train_dataloader, model, n_epochs, learning_rate=0.001,\n",
        "               print_every=100, plot_every=100):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss = train_epoch(train_dataloader, model, optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if epoch % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, epoch / n_epochs),\n",
        "                                        epoch, epoch / n_epochs * 100, print_loss_avg))\n",
        "\n",
        "        if epoch % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8bewjTmCs1Q"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fYMR8GW1F1gj",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Testing Encoder-Decoder with Attention and RNN on one batch of dataset\n",
        "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "decoder = DecoderAttn(output_lang.n_words, hidden_size).to(device)\n",
        "model = EncoderDecoder(encoder, decoder, target_len=10)\n",
        "\n",
        "for data in train_dataloader:\n",
        "    input_tensor, target_tensor = data\n",
        "    decoder_outputs = model(input_tensor, target_tensor)\n",
        "    print(target_tensor.view(-1).size())\n",
        "    print(decoder_outputs.size())\n",
        "    print(f\"target_tensor: {target_tensor.view(-1).size()}, decoder_outputs: {decoder_outputs.view(-1, decoder_outputs.size(-1)).size()}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessed embeddings dataloader\n",
        "#input_lang, output_lang, train_dataloader, context_embeddings_tensor = get_dataloader(batch_size=32)"
      ],
      "metadata": {
        "id": "MTLZ6ERWN-lG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Simple Encoder-Decoder with preprocessed embeddings\n",
        "\n",
        "# encoder = Encoder(context_embeddings_tensor, hidden_dim=hidden_size).to(device)\n",
        "# decoder = Decoder(pretrained_embeddings=context_embeddings_tensor, output_size=output_lang.n_words, hidden_dim=hidden_size).to(device)\n",
        "# model = EncoderDecoder(encoder, decoder, target_len=10)\n",
        "\n",
        "# for data in train_dataloader:\n",
        "#     input_tensor, target_tensor = data\n",
        "#     decoder_outputs = model(input_tensor, target_tensor)\n",
        "#     print(target_tensor.view(-1).size())\n",
        "#     print(decoder_outputs.size())\n",
        "#     print(f\"target_tensor: {target_tensor.view(-1).size()}, decoder_outputs: {decoder_outputs.view(-1, decoder_outputs.size(-1)).size()}\")\n",
        "#     break"
      ],
      "metadata": {
        "id": "Z930gq6LLL5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1VpUdEC9Cev"
      },
      "outputs": [],
      "source": [
        "hidden_size = 126\n",
        "batch_size = 32\n",
        "\n",
        "#input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
        "\n",
        "# Dataloader for preprocessed embeddings\n",
        "input_lang, output_lang, train_dataloader, context_embeddings_tensor = get_dataloader(batch_size=batch_size)\n",
        "# Simple Encoder-Decoder with preprocessed embeddings\n",
        "encoder = Encoder(context_embeddings_tensor, hidden_dim=hidden_size).to(device)\n",
        "decoder = Decoder(pretrained_embeddings=context_embeddings_tensor, output_size=output_lang.n_words, hidden_dim=hidden_size).to(device)\n",
        "model = EncoderDecoder(encoder, decoder, target_len=10)\n",
        "\n",
        "# Encoder-Decoder with Attention and RNN (This is the best model so far)\n",
        "#encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
        "#decoder = DecoderAttn(output_lang.n_words, hidden_size).to(device)\n",
        "#model = EncoderDecoder(encoder, decoder, target_len=10)\n",
        "\n",
        "# Encoder-Decoder Self-Attention\n",
        "#encoder = EncoderSelfAttn(n_features=input_lang.n_words, d_model=hidden_size, n_heads=4, ff_units=4).to(device)\n",
        "#decoder = DecoderSelfAttn(n_features=output_lang.n_words, d_model=hidden_size, n_heads=4, ff_units=4).to(device)\n",
        "#model = EncoderDecoderSelfAttention(encoder, decoder, target_len=10)\n",
        "\n",
        "# Transformer\n",
        "# enclayer = EncoderLayer(d_model=hidden_size, n_heads=3, ff_units=20)\n",
        "# enctransf = EncoderTransf(enclayer, n_features=input_lang.n_words, n_layers=1)\n",
        "# declayer = DecoderLayer(d_model=hidden_size, n_heads=3, ff_units=20)\n",
        "# dectransf = DecoderTransf(decoder_layer=declayer, output_features=output_lang.n_words, n_layers=1)\n",
        "# model = EncoderDecoderSelfAttentionTransformer(enctransf, dectransf, output_features=output_lang.n_words, target_len=10)\n",
        "\n",
        "train(train_dataloader, model, 80, print_every=5, plot_every=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-aAtvPMkRq6"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, sentence, input_lang, output_lang):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "\n",
        "        decoder_outputs = model(input_tensor)\n",
        "\n",
        "        _, topi = decoder_outputs.topk(1)\n",
        "        decoded_ids = topi.squeeze()\n",
        "\n",
        "        decoded_words = []\n",
        "        for idx in decoded_ids:\n",
        "            if idx.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            decoded_words.append(output_lang.index2word[idx.item()])\n",
        "    return decoded_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qv_tn_KGq8Ph"
      },
      "outputs": [],
      "source": [
        "model.eval() # Set to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMc0XCgzmNIp"
      },
      "outputs": [],
      "source": [
        "n = 10\n",
        "print(\"French to English:\\n\")\n",
        "for i in range(n):\n",
        "    pair = random.choice(pairs)\n",
        "    print(f\"{pair[0]} -> {' '.join(evaluate(model, pair[0], input_lang, output_lang))}, CORRECT TRANSLATION: {pair[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uju21ddCpbFZ"
      },
      "outputs": [],
      "source": [
        "' '.join(evaluate(model, 'je vais bien', input_lang, output_lang))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBJJnv4urq7Y"
      },
      "source": [
        "### _Note: I have not included < unk >. If word does not exist already in the embedding it will throw an error._"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ZFiDgFY43Cef",
        "CFdjE0eC2l7T",
        "RlfdNQuKhDTY",
        "ZTOl8Wvbg4-f"
      ],
      "provenance": [],
      "private_outputs": true,
      "mount_file_id": "16VtuTCM42NEDvH853yLa_jFMlp1lg6Ud",
      "authorship_tag": "ABX9TyPQpEm8H/aP8259D2tSxNGj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}